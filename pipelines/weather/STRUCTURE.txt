================================================================================
WEATHER DATA PIPELINE - COMPLETE STRUCTURE
================================================================================

PROJECT OVERVIEW
================

Clean, modular weather data pipeline with:
- Core Python library for OpenWeatherMap API extraction
- Databricks notebooks for cloud orchestration
- Delta Lake storage for persistence
- Automated deployment script
- Single Databricks job with 2-task dependency chain

FOLDER STRUCTURE
================

weather/
├── __init__.py                 # Package initialization
├── deploy.py                   # Deployment automation (176 lines)
│
├── core/
│   ├── __init__.py
│   └── pipeline.py             # Core extraction logic (117 lines)
│                              # Classes: OpenWeatherAPIClient, OpenWeatherPipeline
│
├── notebooks/
│   ├── __init__.py
│   ├── extract_weather.py      # Databricks extract notebook (77 lines)
│   └── load_weather.py         # Databricks load notebook (86 lines)
│
└── config/
    └── job_config.json         # Databricks job definition

FILE DETAILS
============

core/pipeline.py
----------------
Purpose: Reusable core logic independent of Databricks
Classes:
  - OpenWeatherAPIClient: HTTP client for API calls
    * get_weather(city, units): Fetch raw weather data
  - OpenWeatherPipeline: Main orchestrator
    * __init__(api_key): Initialize with API key
    * extract_cities(cities, units): Extract multiple cities
    * _transform_record(api_response): Convert API response to standard format
    * validate(): Data quality validation
    * get_data(): Return extracted data

Format: Python 3.12+ with type hints
Dependencies: requests, logging, typing

notebooks/extract_weather.py
----------------------------
Purpose: Databricks notebook for weather API extraction
Process:
  1. Retrieve API key from Databricks secret scope "dex/openweather_api_key"
  2. Extract weather for: London, New York, Tokyo, Sydney
  3. Transform API responses to standard format
  4. Save to temporary Delta table "weather_raw_temp"
Configuration: Hardcoded cities, metric units
Output: Delta table "weather_raw_temp" with 10 columns
        (city, country, temperature, feels_like, humidity, pressure,
         condition, wind_speed, visibility, timestamp)

notebooks/load_weather.py
------------------------
Purpose: Databricks notebook for Delta Lake loading
Process:
  1. Read from "weather_raw_temp" table (created by extract)
  2. Cast all columns to proper types
  3. Write to "weather_current" table in overwrite mode
Configuration: 10-column schema with type safety
Target Table: "weather_current" (persists across runs)
Dependencies: Requires extract_weather task to complete first

config/job_config.json
----------------------
Purpose: Databricks multi-task job definition
Job Name: weather_data_sync
Tasks:
  1. extract_weather
     - Notebook: /Shared/weather/extract_weather
     - Timeout: 1800 seconds
  2. load_weather (depends on extract_weather)
     - Notebook: /Shared/weather/load_weather
     - Timeout: 1800 seconds
Job Config:
  - Max concurrent runs: 1
  - Total job timeout: 7200 seconds
  - Tags: production/weather/openweathermap

deploy.py
---------
Purpose: Automation script for deploying to Databricks
Class: WeatherPipelineDeployer
Methods:
  - check_auth(): Verify Databricks connectivity
  - upload_notebook(): Deploy individual notebooks
  - deploy_notebooks(): Upload all notebooks to workspace
  - create_job(): Create/update Databricks job
  - deploy_all(): Full deployment workflow
CLI Arguments:
  - --job-id: Update existing job (optional)
  - --check-auth: Test authentication only
Usage: python weather/deploy.py [--check-auth] [--job-id ID]

SECRETS CONFIGURATION
====================
Scope: dex
Key: openweather_api_key
Value: (stored securely in Databricks)
Accessed in notebooks via: dbutils.secrets.get(scope="dex", key="openweather_api_key")

DATABRICKS DEPLOYMENT
====================
Workspace Location: /Shared/weather/
  - extract_weather notebook
  - load_weather notebook

Job ID: 577756171636840
Job Name: weather_data_sync
Status: Deployed and tested (successful run)

EXECUTION FLOW
=============
1. extract_weather task runs
   - Fetches current weather for 4 cities from OpenWeatherMap API
   - Creates DataFrame with 10 standardized columns
   - Writes to temporary Delta table "weather_raw_temp"
   - Time: ~40 seconds

2. load_weather task runs (after extract completes)
   - Reads from "weather_raw_temp" table
   - Applies schema and type casting
   - Writes to persistent Delta table "weather_current"
   - Time: ~15 seconds

Total Job Time: ~60 seconds
Data Retention: weather_current table persists across runs


FEATURES
========
✓ Modular code architecture
✓ Core logic reusable outside Databricks
✓ Type-safe transformations
✓ Error handling and logging
✓ Secrets management via Databricks scopes
✓ Multi-task job orchestration
✓ Delta Lake persistence
✓ Serverless compute compatible
✓ No widgets (hardcoded config)
✓ Automated deployment script

QUICK START
===========
Deploy to Databricks:
  python pipelines/weather/deploy.py

Test authentication:
  python pipelines/weather/deploy.py --check-auth

Update existing job:
  python pipelines/weather/deploy.py --job-id 577756171636840

Run locally (test core pipeline):
  from pipelines.weather.core import OpenWeatherPipeline
  pipeline = OpenWeatherPipeline(api_key)
  data = pipeline.extract_cities(["London", "New York"])
  pipeline.validate()

DATABASE SCHEMA
===============
Table: weather_current
Columns:
  - city: STRING
  - country: STRING
  - temperature: DOUBLE (°C)
  - feels_like: DOUBLE (°C)
  - humidity: INTEGER (%)
  - pressure: INTEGER (hPa)
  - condition: STRING (e.g., "Cloudy")
  - wind_speed: DOUBLE (m/s)
  - visibility: DOUBLE (meters)
  - timestamp: STRING (ISO format)

Sample Row:
  London | United Kingdom | 8.5 | 6.2 | 72 | 1013 | Overcast | 4.5 | 10000 | 2026-01-12T...

MONITORING
==========
View job runs: https://dbc-23ee5bc9-bf5e.cloud.databricks.com/
Job ID: 577756171636840
Check Databricks UI for:
  - Task execution logs
  - Performance metrics
  - Data lineage
  - Error messages

ENVIRONMENT REQUIREMENTS
=======================
Local Development:
  - Python 3.12+
  - requests library
  - uv (environment management)
  - Poetry (project metadata only)

Databricks:
  - Workspace with serverless compute
  - Databricks CLI v0.18.0+
  - Personal access token configured
  - Secret scope "dex" with "openweather_api_key"

API Requirements:
  - OpenWeatherMap API key (5-day forecast free tier)
  - Active internet connection for API calls
